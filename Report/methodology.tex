\section{Methodology}
In this project, we used WordNet as the main lexical database. Using WordNet, we were able to extract the part-of-speech tags for a word as well as their synset (synonym set). This played an important role in determining the similarity of 2 words as we would see later.

\subsection{Data Preparation}
The training data provided contained pairs of questions and indicators to determine if the two questions are similar. In the first model, the data was first tokenized into list of words and stemmed to their base form. As a word could be associated to multiple synset, we used Lesk algorithm to choose the correct synset given the context of the sentence. In Lesk algorithm, for each possible synset we collected its hypernyms and hyponyms into  a list. We then counted the number of common words from this list with the remaining words in the sentence. The synset with the highest number of overlap will then be chosen to be the best synset for the word. Once this was done, stop words were removed and the tokens were sent to the function which computed the semantic and word order score of the 2 sentences. Please see appendix for more details on how to calculate these measures. 

\subsection{Training}
To train the data we used the measures from the above procedure as the features. The original model proposed in [] was designed to map each feature to be between 0 and 1. There was also a constraint put on the weights to ensure that the sum of the weights was 1. However, we tried to applying perceptron learning in order to automate the learning process. The perceptron initialized the weights to be small random variable with 0.1 learning rate and 1000 iteration.