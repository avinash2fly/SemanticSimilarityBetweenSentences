\section{Methodology}
In this project, we used WordNet as the main lexical database. Using WordNet, we were able to extract the part-of-speech tags for a word as well as their synset (synonym set). This played an important role in determining the similarity of 2 words.

\subsection{Data Preparation}
The training data provided contained pairs of questions and indicators to determine if the two questions were similar. In the first model, each sentence was tokenized into list of words and each word was stemmed to their base form. As a word could be associated to multiple synsets, we used Lesk algorithm to choose the correct synset given the context of the sentence. In Lesk algorithm, for each possible synset we collected its hypernyms and hyponyms into  a list. We then counted the number of common words from this list with the remaining words in the sentence. The synset with the highest number of overlap will then be chosen to be the best synset for the word. Once this was done, stop words were removed and the tokens were sent to a function which computed the semantic similarity and word order score of the 2 sentences. Please see appendix for more details on how to calculate these measures. 

For the LSTM model, the input for the neural network is the individual word in the sentences. One way to translate words into features that could be accepted by machines is to use one hot encoding, with which each word would be translated into a boolean vector whose length equals the size of the vocabulary. Due to the large size of English vocabulary, dimension reduction methods need to be applied on the resulted boolean vector. 

A better way to represent words would be to project each word into a real valued vector space. This method is invented by Google and has proven success in literatures. Hence, we adopt this method to preprocess our data. After preprocessing, each question would be represented by a list of vectors, each of which representing the original word.

\subsection{Training \& Model Selection}
To train the data we used the measures from the above procedure as the features. The original model proposed in (Li et al. 2016) was designed to map each feature to be between 0 and 1. There was also a constraint put on the weights to ensure that the sum of the weights was 1. We tried perceptron learning in order to automate the learning process for the weight. The perceptron initialized the weights to be small random variable with 0.1 learning rate and 1000 iteration. However, we found that the parameters did not converge even after 2000 iterations. By plotting the data against the semantic similarity score and the word order score, it appeared that the data is not linearly separable thus a non-linear model was required. 

Although the bag of words with Naive Bayes seems to be an ideal choice for this problem, because we are dealing with short sentences, these models will not work well as they rely on pure statistics of word frequencies.
After some research, we decided that the LSTM recursive neural network would be the best model for measuring semantic similarity. This model provided a similarity measure afterwhich we could learn the optimal decision boundary to decide whether or not the question pair was a duplicate.