\section{Methodology}
In this project, we used WordNet as the main lexical database. Using WordNet, we were able to extract the part-of-speech tags for a word as well as their synset (synonym set). This played an important role in determining the similarity of 2 words as we would see later.

\subsection{Data Preparation}
The training data provided contained pairs of questions and indicators to determine if the two questions are similar. In the first model, the data was first tokenized into list of words and stemmed to their base form. As a word could be associated to multiple synset, we used Lesk algorithm to choose the correct synset given the context of the sentence. In Lesk algorithm, for each possible synset we collected its hypernyms and hyponyms into  a list. We then counted the number of common words from this list with the remaining words in the sentence. The synset with the highest number of overlap will then be chosen to be the best synset for the word. Once this was done, stop words were removed and the tokens were sent to the function which computed the semantic and word order score of the 2 sentences. Please see appendix for more details on how to calculate these measures. 

For the LSTM model, the input for the neural network is the individual word in the sentences. One way to translate words into features that could be accepted by machines is to use one hot encoding, with which each word would be translated into a boolean vector whose length equals the size of the vocabulary. Due to the large size of English vocabulary, dimension reduction methods need to be applied on the resulted boolean vector as well. 

A better way to represent words could be to project each word into a real valued vector space. This method is invented by Google and has proven success in the literature. Hence, we adopt this method to preprocess our data. After preprocessing, each question would be represented by a list of vectors, each of which representing the original word.

\subsection{Training & Model Selection}
To train the data we used the measures from the above procedure as the features. The original model proposed in (Aditya, 2016) was designed to map each feature to be between 0 and 1. There was also a constraint put on the weights to ensure that the sum of the weights was 1. However, we tried perceptron learning in order to automate the learning process for the weight. The perceptron initialized the weights to be small random variable with 0.1 learning rate and 1000 iteration. However, 