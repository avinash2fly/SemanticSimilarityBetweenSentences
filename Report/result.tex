\section{Result \& Discussion}


\subsection{Challenges}
The first challenge we face is, what should our vocabulary be, if we were to project the vocabulary into a vector space ourselves. One way is to use all words in the training set, but since embedding requires huge amounts of data to be accurate, this can't work well. Another way could be, to retrieve all questions on Quora, and use the words in all those questions as the vocabulary. The problem of this method is, we would actually be peeking into the testing set, since inevitably the testing set would be from Quora. A third method would be, to use some third party source to construct the vocabulary and the corresponding word vectors.

We believe the third method is the best, and hence we use the Google News vectors (https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM) as our embedding.

The second challenge is that, due to the computational complexity and hardware dependencies of the model, specifically the requirements of the theano package, we were not able to replicate the training of the LSTM model ourselves before the deadline of the project. Instead, we used a pre-trained model to do the prediction, namely the Siaseme LSTM (https://github.com/aditya1503/Siamese-LSTM) model trained based on the sentences involving compositional knowledge dataset (Marelli et al 2014) as a proxy. We feed the question pairs into the Siaseme model, and it would produce the similarity measure between them. We then run a simple test to find the threshold beyond which we should predict duplicate.

If we were to train the LSTM RNN ourselves, we would feed it with our labelled data, in the form of (question 1, question2, label) where label is a boolean indicating whether the two questions are duplicate or not, and the test data would be in the form of (question 1, question2). This way, the model performance would be much better.